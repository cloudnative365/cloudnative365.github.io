云原生应用部署

完成下面的应用部署您可能需要使用现有的技术知识就可完成，也可能需要在短时间内快速学习Kubernetes等云原生一些技术知识才能完成。

如果您在练习过程中，遇到账户登录及使用问题，欢迎与***取得联系。非账户登录及使用问题需要自行尝试解决。

 

# **基本信息：**

1. 本练习主要完成在虚机中部署应用。

2. 提供两台VM，配置相同：2 OCPU，32G Memory，100G存储

3. 提供两台虚机的public和private ip地址，ssh key，使用opc用户访问，opc有sudo权限

4. 从虚机里可以访问公网

5. 虚机所在的虚拟网络已经打开了的端口：22，443，80，6379，30080，如需其它端口，可以跟我们联系。

| 虚机名 | OS              | Public IP     | Private IP |
| ------ | --------------- | ------------- | ---------- |
| VM1    | Oracle Linux7.9 | 138.2.84.101  | 10.0.0.185 |
| VM2    | Oracle Linux7.9 | 129.150.63.43 | 10.0.0.117 |

# **具体要求：**

## 一、部署kubernetes集群

### 1. 部署Kubernetes集群。

### 2. 提交物

#### a.   简要安装步骤

+ 初始化两台机器的环境

  ``` bash
  # 加载模块
  $ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
  overlay
  br_netfilter
  EOF
  sudo modprobe overlay
  sudo modprobe br_netfilter
  # 验证模块是否生效
  $ lsmod | grep br_netfilter
  br_netfilter           24576  0
  bridge                172032  1 br_netfilter
  #新建k8s.conf文件，并添加以下内容，这个是防止由于 iptables 被绕过而导致流量无法正确路由的问题。
  $ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
  net.bridge.bridge-nf-call-ip6tables = 1
  net.bridge.bridge-nf-call-iptables = 1
  net.ipv4.ip_forward = 1
  EOF
  #执行修改的桥接网络设置
  $ sudo sysctl -p /etc/sysctl.d/k8s.conf
  # 验证桥接的参数
  $ ls /proc/sys/net/bridge
  bridge-nf-call-arptables  bridge-nf-call-iptables        bridge-nf-filter-vlan-tagged
  bridge-nf-call-ip6tables  bridge-nf-filter-pppoe-tagged  bridge-nf-pass-vlan-input-dev
  # 把SELinux改成permissive模式
  $ sudo setenforce 0
  $ sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
  # 防火墙相关
  $ sudo iptables -F
  $ sudo systemctl stop firewalld
  $ sudo systemctl disable firewalld
  # 初始化repo源
  $ sudo yum install -y epel-release wget telnet
  $ sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  # 安装docker
  $ sudo yum install -y docker
  $ sudo systemctl enable docker.service
  $ sudo systemctl start docker.service
  # 配置kubernetes相关依赖
  $ cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
  [kubernetes]
  name=Kubernetes
  baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
  enabled=1
  gpgcheck=1
  gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
  exclude=kubelet kubeadm kubectl
  EOF
  # 安装kubernetes相关软件
  $ sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
  $ sudo systemctl enable --now kubelet
  $ sudo swapoff -a
  ```

#### b.   Kubernetes管理节点

+ master节点

  ``` bash
  # 初始化master节点
  sudo kubeadm init --apiserver-advertise-address=10.0.0.185
  ```

  出现下面的结果算是成功初始化

  ``` bash
  Your Kubernetes master has initialized successfully!
  
  To start using your cluster, you need to run the following as a regular user:
  
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
  
  You should now deploy a pod network to the cluster.
  Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/
  
  You can now join any number of machines by running the following on each node
  as root:
  
    kubeadm join 10.0.0.185:6443 --token donhe3.syfx76a11herbqf5 --discovery-token-ca-cert-hash sha256:4f0ff9b75916d4875834f62f34064f32419c3b1b4ca6eabdbaf3357c689949ca
  ```
  
  最后执行
  
  ``` bash
  $ mkdir -p $HOME/.kube
  $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  $ sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```

+ node节点上加入master节点的操作同样会有报错，我们按照下面的步骤来执行

  ``` bash
  sudo kubeadm join 10.0.1.12:6443 --token upcade.d7gjcd27emdgqtyc --discovery-token-ca-cert-hash sha256:a99fbf855fe8703ca5df8369fe21f1694ef5b1b7a86b4391c7391ba3d794a5c0
  ```
  
+ 这个时候集群依然无法启动，因为没有网络，我们回到master节点上，我们使用weave作为网络插件

  ``` bash
  kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
  ```

#### c.   相应截图，如：集群状态，Services状态，Pod状态截图

成功后状态如下

+ 集群状态

  ![image-20220705134520248](/Users/jormun/Library/Application Support/typora-user-images/image-20220705134520248.png)

+ Pod状态

  ![image-20220705134415945](/Users/jormun/Library/Application Support/typora-user-images/image-20220705134415945.png)

+ Service状态

  ![image-20220705134459307](/Users/jormun/Library/Application Support/typora-user-images/image-20220705134459307.png)

## **二、**    **部署MySQL**

### 1. 在上述集群上以有状态应用方式部署MySQL，挂载pv

### 2. 创建sample数据库和sample表，插入几条记录

### 3. 提交物：

#### a.   简要安装步骤

+ 安装nfs提供存储

  ``` bash
  # 安装nfs
  $ sudo yum -y install nfs-utils
  #创建nfs目录
  $ sudo mkdir -p /mnt/nfs
  #修改权限
  $ sudo chmod -R 777 /mnt/nfs
  #编辑export文件
  $ sudo vim /etc/exports
  /mnt/nfs *(rw,no_root_squash,sync)
  #配置生效
  $ sudo exportfs -r
  #查看生效
  $ sudo exportfs
  #启动rpcbind、nfs服务
  $ sudo systemctl restart rpcbind && sudo systemctl enable rpcbind
  $ sudo systemctl restart nfs && sudo systemctl enable nfs
  #查看 RPC 服务的注册状况
  $ sudo rpcinfo -p localhost
  #showmount测试
  $ sudo showmount -e 10.0.0.185
  #所有node节点安装客户端
  $ sudo yum -y install nfs-utils
  $ sudo systemctl start nfs && sudo systemctl enable nfs
  ```

+ 使用nfs-provisioner来提供sc，https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

  ``` bash
  $ wget https://get.helm.sh/helm-v3.9.0-linux-amd64.tar.gz
  $ tar xf helm-v3.9.0-linux-amd64.tar.gz
  $ sudo mv linux-amd64/helm /usr/local/sbin
  $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
  $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
      --set nfs.server=10.0.0.185 \
      --set nfs.path=/mnt/nfs
  ```

+ 创建statefulset

  ``` bash
  apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    labels:
      app: mysql-master
    name: mysql-master
  spec:
    replicas: 1
    serviceName: mysql
    selector:
      matchLabels:
        app: mysql-master
    template:
      metadata:
        labels:
          app: mysql-master
      spec:
        containers:
          - env:
              - name: MYSQL_ROOT_PASSWORD
                value: 'mysql'
            image: 'mysql:5.7'
            imagePullPolicy: Always
            name: mysql-master
            ports:
              - containerPort: 3306
                name: mysql-master
                protocol: TCP
            volumeMounts:
              - mountPath: /var/lib/mysql
                name: pv-nfs-mysql-master
        volumes:
        - name: pv-nfs-mysql-master
          persistentVolumeClaim:
            claimName: mysql-pvc
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: mysql
    name: mysql
  spec:
    ports:
      - name: mysql-master
        port: 3306
        protocol: TCP
    selector:
      app: mysql-master
    type: ClusterIP
  ---
  kind: PersistentVolume
  apiVersion: v1
  metadata:
    name: k8s-pv-my1
    labels:
      type: mysql
  spec:
    capacity:
      storage: 20Gi
    storageClassName: mysql
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: "/var/lib/mysql"
    persistentVolumeReclaimPolicy: Retain
  ---
  kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: mysql-pvc
  spec:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 20Gi
    storageClassName: mysql
  ```

#### b.   MySQL root密码

进入容器内部

``` bash
kubectl exec -it mysql-master-0 -- /bin/sh
```

获得密码

``` bash
echo $MYSQL_ROOT_PASSWORD
mysql
```

#### c.   相应的截图

+ mysql root密码在传递容器变量的时候通过下面的方式传递给了mysql

``` bash
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: mysql
```

+ 创建sample数据库和sample表，插入几条记录

  由于10250端口未开放，在master上执行exec命令有问题

  ![image-20220705143729413](/Users/jormun/Library/Application Support/typora-user-images/image-20220705143729413.png)

  登录到vm2上使用docker来操作

  ``` bash
  CREATE DATABASE sample;
  CREATE TABLE sample.sample (message VARCHAR(250));
  INSERT INTO sample.sample VALUES ('hello');
  ```

  

  ![image-20220705143923836](/Users/jormun/Library/Application Support/typora-user-images/image-20220705143923836.png)



## **三、**    **部署应用Wordpress**

### 1. 在上述集群上以无状态应用方式部署Wordpress，不挂载pv。

### 2. 部署 Wordpress 以 NodePort 方式对外以30080端口公开。

### 3. 通过公网访问 Wordpress，成功发布一条博文。

### 4. 提交成果物

#### a.   简要步骤

+ 部署 Wordpress 以 NodePort 方式对外以30080端口公开

  ``` bash
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: wordpress
    name: wordpress
  spec:
    ports:
      - name: wordpress
        nodePort: 30080
        port: 80
        protocol: TCP
        targetPort: 80
    selector:
      app: wordpress
    type: NodePort
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: wordpress
    labels:
      app: wordpress
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: wordpress
    template:
      metadata:
        labels:
          app: wordpress
      spec:
        containers:
        - image: wordpress
          name: wordpress
          env:
          - name: WORDPRESS_DB_NAME
            value: sample
          - name: WORDPRESS_DB_USER
            value: root
          - name: WORDPRESS_DB_PASSWORD
            value: mysql
          - name: WORDPRESS_DB_HOST
            value: 10.32.0.2 # 由于安全组的限制，node节点上的pod无法使用在master节点上的CoreDNS解析，所以使用dns名字会报错，但是题目并没有限制我，所以我这里直接使用了IP地址
  ```


#### b.   Wordpress 网站的首页截图，截图里需要包括您发布的博文。

+ 配置成功截图

  ![image-20220705145731629](/Users/jormun/Library/Application Support/typora-user-images/image-20220705145731629.png)

+ 登录进去是这样的

  ![image-20220705150315361](/Users/jormun/Library/Application Support/typora-user-images/image-20220705150315361.png)

+ 发布文章

  ![image-20220705151509215](/Users/jormun/Library/Application Support/typora-user-images/image-20220705151509215.png)

+ 首页截图

  ![image-20220705151455657](/Users/jormun/Library/Application Support/typora-user-images/image-20220705151455657.png)

## 四、部署Redis应用

### 1. 在k8s里或虚机上部署一个Redis服务

### 2. 连接到Redis，设置一个key-value，并查询出来

### 3. 提交物

#### a.   简要步骤

+ redis.yml

  ``` bash
  ---
  kind: ConfigMap
  apiVersion: v1
  metadata:
    name: redis-config
    labels:
      app: redis
  data:
    redis.conf: |-
      dir /data
      port 6379
      bind 0.0.0.0
      appendonly yes
      protected-mode no
      requirepass jormun
      pidfile /data/redis-6379.pid
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: redis
    labels:
      app: redis
  spec:
    type: ClusterIP
    ports:
      - name: redis
        port: 6379
    selector:
      app: redis
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: redis
    labels:
      app: redis
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: redis
    template:
      metadata:
        labels:
          app: redis
      spec:
        containers:
          - name: redis
            image: redis
            command:
              - "sh"
              - "-c"
              - "redis-server /usr/local/etc/redis/redis.conf"
            ports:
              - containerPort: 6379
            volumeMounts:
              - name: config
                mountPath: /usr/local/etc/redis/redis.conf
                subPath: redis.conf
        volumes:
          - name: config
            configMap:
              name: redis-config
  ```

#### b.   Reids服务的密码，设置的key-value值

密码可以在configmap中找到，由于kubelet的10250端口不通，直接到node节点上使用docker来做

key-value值

``` bash
kubectl exec -it redis-6dbf487fc9-cwb2k -- /bin/sh
$ redis-cli
127.0.0.1:6379> auth jormun
OK
127.0.0.1:6379> set Student:1:name Tom
OK
127.0.0.1:6379> get Student:1:name
"Tom"
127.0.0.1:6379>
```

#### c.   相应的截图

![image-20220705152450929](/Users/jormun/Library/Application Support/typora-user-images/image-20220705152450929.png)