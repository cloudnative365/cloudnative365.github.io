云原生应用部署

完成下面的应用部署您可能需要使用现有的技术知识就可完成，也可能需要在短时间内快速学习Kubernetes等云原生一些技术知识才能完成。

如果您在练习过程中，遇到账户登录及使用问题，欢迎与***取得联系。非账户登录及使用问题需要自行尝试解决。

 

# **基本信息：**

1. 本练习主要完成在虚机中部署应用。

2. 提供两台VM，配置相同：2 OCPU，32G Memory，100G存储

3. 提供两台虚机的public和private ip地址，ssh key，使用opc用户访问，opc有sudo权限

4. 从虚机里可以访问公网

5. 虚机所在的虚拟网络已经打开了的端口：22，443，80，6379，30080，如需其它端口，可以跟我们联系。

| 虚机名 | OS         | Public IP     | Private IP |
| ------ | ---------- | ------------- | ---------- |
| VM1    | CentOS 7.9 | 138.2.84.101  | 10.0.1.11  |
| VM2    | CentOS 7.9 | 129.150.63.43 | 10.0.0.12  |

# **具体要求：**

## 一、部署kubernetes集群

### 1. 部署Kubernetes集群。

### 2. 提交物

#### a.   简要安装步骤

+ 初始化两台机器的环境

  ``` bash
  # 加载模块
  $ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
  overlay
  br_netfilter
  EOF
  sudo modprobe overlay
  sudo modprobe br_netfilter
  # 验证模块是否生效
  $ lsmod | grep br_netfilter
  br_netfilter           24576  0
  bridge                172032  1 br_netfilter
  #新建k8s.conf文件，并添加以下内容，这个是防止由于 iptables 被绕过而导致流量无法正确路由的问题。
  $ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
  net.bridge.bridge-nf-call-ip6tables = 1
  net.bridge.bridge-nf-call-iptables = 1
  net.ipv4.ip_forward = 1
  EOF
  #执行修改的桥接网络设置
  $ sudo sysctl -p /etc/sysctl.d/k8s.conf
  # 验证桥接的参数
  $ ls /proc/sys/net/bridge
  bridge-nf-call-arptables  bridge-nf-call-iptables        bridge-nf-filter-vlan-tagged
  bridge-nf-call-ip6tables  bridge-nf-filter-pppoe-tagged  bridge-nf-pass-vlan-input-dev
  # 把SELinux改成permissive模式
  $ sudo setenforce 0
  $ sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
  # 防火墙相关
  $ sudo iptables -F
  $ sudo systemctl stop firewalld
  $ sudo systemctl disable firewalld
  # 初始化repo源
  $ sudo yum install -y epel-release wget telnet
  $ sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  # 安装docker
  $ sudo yum install -y docker-ce docker-ce-cli containerd.io
  $ sudo systemctl enable docker.service
  $ sudo systemctl start docker.service
  # 配置kubernetes相关依赖
  $ cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
  [kubernetes]
  name=Kubernetes
  baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
  enabled=1
  gpgcheck=1
  gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
  exclude=kubelet kubeadm kubectl
  EOF
  # 安装kubernetes相关软件
  $ sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
  $ sudo systemctl enable --now kubelet
  $ sudo swapoff -a
  ```

#### b.   Kubernetes管理节点

+ master节点

  ``` bash
  # 初始化master节点
  sudo kubeadm init --apiserver-advertise-address=10.0.1.12 --pod-network-cidr=192.168.0.0/16
  ```

  会出现下面的报错

  > sudo kubeadm init --apiserver-advertise-address=10.0.1.1 --pod-network-cidr=10.244.0.0/16 
  > [init] Using Kubernetes version: v1.23.0
  > [preflight] Running pre-flight checks
  > error execution phase preflight: [preflight] Some fatal errors occurred:
  > [ERROR CRI]: container runtime is not running: output: E0704 06:28:01.208226    9685 remote_runtime.go:925] "Status from runtime service failed" err="rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
  > time="2022-07-04T06:28:01Z" level=fatal msg="getting status of runtime: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
  > , error: exit status 1
  > 	[ERROR KubeletVersion]: the kubelet version is higher than the control plane version. This is not a supported version skew and may lead to a malfunctional cluster. Kubelet version: "1.24.2" Control plane version: "1.23.0"
  > [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
  > To see the stack trace of this error execute with --v=5 or higher

​		这是一个已知的issue，issue id看这个https://github.com/containerd/containerd/issues/4581，临时解决方法如下

``` bash
sudo rm -rf /etc/containerd/config.toml
sudo systemctl restart containerd
sudo kubeadm init --apiserver-advertise-address=10.0.1.1 --pod-network-cidr=10.244.0.0/16
```

出现下面的结果算是成功初始化

``` bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.1.12:6443 --token upcade.d7gjcd27emdgqtyc \
	--discovery-token-ca-cert-hash sha256:a99fbf855fe8703ca5df8369fe21f1694ef5b1b7a86b4391c7391ba3d794a5c0
```

最后执行

``` bash
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

+ node节点上加入master节点的操作同样会有报错，我们按照下面的步骤来执行

  ``` bash
  sudo rm -rf /etc/containerd/config.toml
  sudo systemctl restart containerd
  sudo kubeadm join 10.0.1.12:6443 --token upcade.d7gjcd27emdgqtyc --discovery-token-ca-cert-hash sha256:a99fbf855fe8703ca5df8369fe21f1694ef5b1b7a86b4391c7391ba3d794a5c0
  ```

+ 这个时候集群依然无法启动，因为没有网络，我们回到master节点上，我们使用weave作为网络插件

  ``` bash
  kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
  ```

#### c.   相应截图，如：集群状态，Services状态，Pod状态截图



## **二、**    **部署MySQL**

### 1. 在上述集群上以有状态应用方式部署MySQL，挂载pv

### 2. 创建sample数据库和sample表，插入几条记录

### 3. 提交物：

#### a.   简要安装步骤

+ 安装nfs提供存储

  ``` bash
  # 安装nfs
  yum -y install nfs-utils
  #创建nfs目录
  mkdir -p /app/nfs
  #修改权限
  chmod -R 777 /app/nfs
  #编辑export文件
  vim /etc/exports
  /app/nfs *(rw,no_root_squash,sync)
  #配置生效
  exportfs -r
  #查看生效
  exportfs
  #启动rpcbind、nfs服务
  systemctl restart rpcbind && systemctl enable rpcbind
  systemctl restart nfs && systemctl enable nfs
  #查看 RPC 服务的注册状况
  rpcinfo -p localhost
  #showmount测试
  showmount -e 192.168.90.9
  #所有node节点安装客户端
  yum -y install nfs-utils
  systemctl start nfs && systemctl enable nfs
  ```

+ 使用nfs-provisioner来提供sc，https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

  ``` bash
  $ wget https://get.helm.sh/helm-v3.9.0-linux-amd64.tar.gz
  $ tar xf helm-v3.9.0-linux-amd64.tar.gz
  $ mv linux-amd64/helm /usr/local/sbin
  $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
  $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
      --set nfs.server=10.0.1.12 \
      --set nfs.path=/app/nfs
  ```

+ 创建statefulset

  ``` bash
  apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations: {}
    labels:
      app: mysql-master
    name: mysql-master
  spec:
    replicas: 1
    serviceName: mysql
    selector:
      matchLabels:
        app: mysql-master
    template:
      metadata:
        labels:
          app: mysql-master
      spec:
        imagePullSecrets:
        - name: ali-secret
        containers:
          - env:
              - name: MYSQL_ROOT_PASSWORD
                value: 'mysql'
            image: 'mysql:5.7'
            imagePullPolicy: Always
            name: mysql-master
            ports:
              - containerPort: 3306
                name: mysql-master
                protocol: TCP
            volumeMounts:
              - mountPath: /var/lib/mysql
                name: pv-nfs-mysql-master
    volumeClaimTemplates:
    - metadata:
        name: pv-nfs-mysql-master
      spec:
        accessModes: [ "ReadWriteMany"]
        resources:
          requests:
            storage: 20G
        storageClassName: nfs-client
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: mysql
    name: mysql
  spec:
    ports:
      - name: mysql-master
        port: 3306
        protocol: TCP
    selector:
      app: mysql-master
    type: ClusterIP
  ```

#### b.   MySQL root密码

进入容器内部

``` bash
kubectl exec -it mysql-master-0 -- /bin/sh
```

获得密码

``` bash
echo $MYSQL_ROOT_PASSWORD
mysql
```

#### c.   相应的截图

创建sample数据库和sample表，插入几条记录

``` bash
CREATE DATABASE sample;
CREATE TABLE sample.sample (message VARCHAR(250));
INSERT INTO sample.sample VALUES ('hello');
```



## **三、**    **部署应用Wordpress**

### 1. 在上述集群上以无状态应用方式部署Wordpress，不挂载pv。

### 2. 部署 Wordpress 以 NodePort 方式对外以30080端口公开。

### 3. 通过公网访问 Wordpress，成功发布一条博文。

### 4. 提交成果物

#### a.   简要步骤

+ 部署 Wordpress 以 NodePort 方式对外以30080端口公开

  ``` bash
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: wordpress
    name: wordpress
  spec:
    ports:
      - name: wordpress
        nodePort: 30080
        port: 80
        protocol: TCP
        targetPort: 80
    selector:
      app: wordpress
    type: NodePort
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: wordpress
    labels:
      app: wordpress
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: wordpress
    template:
      metadata:
        labels:
          app: wordpress
      spec:
        containers:
        - image: wordpress
          name: wordpress
          env:
          - name: WORDPRESS_DB_NAME
            value: sample
          - name: WORDPRESS_DB_USER
            value: root
          - name: WORDPRESS_DB_PASSWORD
            value: mysql
          - name: WORDPRESS_DB_HOST
            value: 192.168.71.213
  ```

  

#### b.   Wordpress 网站的首页截图，截图里需要包括您发布的博文。

 

## 四、部署Redis应用

### 1. 在k8s里或虚机上部署一个Redis服务

### 2. 连接到Redis，设置一个key-value，并查询出来

### 3. 提交物

#### a.   简要步骤

+ 配置文件redis-cm.yml

  ``` bash
  kind: ConfigMap
  apiVersion: v1
  metadata:
    name: redis-config
    labels:
      app: redis
  data:
    redis.conf: |-
      dir /data
      port 6379
      bind 0.0.0.0
      appendonly yes
      protected-mode no
      requirepass jormun
      pidfile /data/redis-6379.pid
  ```

+ redis-deploy.yml

  ``` bash
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: redis
    labels:
      app: redis
  spec:
    type: ClusterIP
    ports:
      - name: redis
        port: 6379
    selector:
      app: redis
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: redis
    labels:
      app: redis
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: redis
    template:
      metadata:
        labels:
          app: redis
      spec:
        containers:
          - name: redis
            image: redis
            command:
              - "sh"
              - "-c"
              - "redis-server /usr/local/etc/redis/redis.conf"
            ports:
              - containerPort: 6379
            volumeMounts:
              - name: config
                mountPath: /usr/local/etc/redis/redis.conf
                subPath: redis.conf
        volumes:
          - name: config
            configMap:
              name: redis-config
  ```

#### b.   Reids服务的密码，设置的key-value值

密码可以在configmap中找到

key-value值

``` bash
kubectl exec -it redis-6945c94d4c-vh9xk -- /bin/sh
# redis-cli
127.0.0.1:6379> auth jormun
OK
127.0.0.1:6379> set Student:1:name Tom
OK
127.0.0.1:6379> get Student:1:name
"Tom"
127.0.0.1:6379>
```

#### c.   相应的截图