<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>用kubeadm搭建k8s高可用（yum版） | cloudnative365.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="用kubeadm搭建k8s高可用（yum版）" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/keynotes_L4_architect_1_HA_1_k8s_cluster_kubeadm_yum.html" />
<meta property="og:url" content="http://localhost:4000/keynotes_L4_architect_1_HA_1_k8s_cluster_kubeadm_yum.html" />
<meta property="og:site_name" content="cloudnative365.github.io" />
<script type="application/ld+json">
{"headline":"用kubeadm搭建k8s高可用（yum版）","url":"http://localhost:4000/keynotes_L4_architect_1_HA_1_k8s_cluster_kubeadm_yum.html","@type":"WebPage","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=481468f1311b237dbb8ec4fec8c1099b04cbcc76">
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="http://localhost:4000/">cloudnative365.github.io</a></h1>
      

      <h2 id="1-介绍">1. 介绍</h2>

<h3 id="11-kubeadm">1.1. kubeadm</h3>

<p>前面讲kubeadm的时候，我们已经介绍过kubeadm的功能的，我么这里只说一下kubeadm创建集群时候的操作</p>

<h3 id="2-架构图">2 架构图</h3>

<h3 id="21-整体架构图">2.1. 整体架构图</h3>

<p><img src="/pages/keynotes/L4_architect/1_HA/pics/1_k8s_cluster_kubeadm_yum/ha-master-gce-2893976.png" alt="ha-master-gce" /></p>

<h2 id="3-资源清单">3. 资源清单</h2>

<h3 id="31-测试环境">3.1. 测试环境</h3>

<table>
  <thead>
    <tr>
      <th>主机</th>
      <th>组件</th>
      <th>CPU</th>
      <th>内存</th>
      <th>磁盘</th>
      <th>操作系统</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10.0.11.18</td>
      <td>etcd1，master1</td>
      <td>2C</td>
      <td>4G</td>
      <td>10G</td>
      <td>RHEL7/8，CentOS7/8</td>
    </tr>
    <tr>
      <td>10.0.12.55</td>
      <td>etcd2，master2</td>
      <td>2C</td>
      <td>4G</td>
      <td>10G</td>
      <td>RHEL7/8，CentOS7/8</td>
    </tr>
    <tr>
      <td>10.0.13.22</td>
      <td>etcd3，master3</td>
      <td>2C</td>
      <td>4G</td>
      <td>10G</td>
      <td>RHEL7/8，CentOS7/8</td>
    </tr>
    <tr>
      <td>10.0.12.16</td>
      <td>worker1</td>
      <td>2C</td>
      <td>4G</td>
      <td>10G</td>
      <td>RHEL7/8，CentOS7/8</td>
    </tr>
    <tr>
      <td>10.0.13.37</td>
      <td>worker2</td>
      <td>2C</td>
      <td>4G</td>
      <td>10G</td>
      <td>RHEL7/8，CentOS7/8</td>
    </tr>
    <tr>
      <td>10.0.1.157</td>
      <td>loadbalancer</td>
      <td>2C</td>
      <td>4G</td>
      <td>10G</td>
      <td>RHEL7/8，CentOS7/8</td>
    </tr>
  </tbody>
</table>

<h3 id="32-生产环境">3.2. 生产环境</h3>

<h2 id="4-安装与配置">4. 安装与配置</h2>

<h3 id="41-初始化">4.1. 初始化</h3>

<ul>
  <li>
    <p>修改机器名</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hostnamectl set-hostname XXX
</code></pre></div>    </div>
  </li>
  <li>
    <p>配置host文件/etc/hosts，在生产系统中，我们通常会使用DNS服务器，但是为了简化，我们这里就使用本地的解析</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10.0.11.18 master1
10.0.12.55 master2
10.0.13.22 master3
10.0.12.16 worker1
10.0.13.37 worker2
10.0.1.157 master-lb
</code></pre></div>    </div>
  </li>
  <li>
    <p>关闭防火墙</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#停止当前防火墙服务</span>
systemctl stop firewalld.service
<span class="c">#禁用防火墙启动</span>
systemctl disable firewalld.service
<span class="c">#查看防火墙状态</span>
firewall-cmd <span class="nt">--state</span>
</code></pre></div>    </div>

    <p><strong><em>注意</em></strong>：firewalld是一个管理工具，他管理的是linux的内核子系统netfilter，关闭firewalld并不意味着禁用了内核子系统，只不过丢给kube-proxy去管理，而kube-proxy是通过管理lvs来管理netfilter规则的，所以说，要关闭firewalld，防止他和lvs产生冲突。</p>
  </li>
  <li>
    <p>关闭selinux</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#关闭当前selinux服务</span>
<span class="nv">$ </span>setenforce 0 
<span class="c">#修改selinux配置文件，防止重启后再次开启</span>
sed <span class="nt">-i</span> <span class="s1">'s/^SELINUX=enforcing$/SELINUX=permissive/'</span> /etc/selinux/config
</code></pre></div>    </div>
  </li>
  <li>
    <p>桥接网络配置</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 加载模块</span>
<span class="nv">$ </span>modprobe br_netfilter
<span class="c"># 验证模块是否生效</span>
<span class="nv">$ </span>lsmod | <span class="nb">grep </span>br_netfilter
br_netfilter           24576  0
bridge                172032  1 br_netfilter
  
<span class="c">#新建k8s.conf文件，并添加以下内容，这个是防止由于 iptables 被绕过而导致流量无法正确路由的问题。</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
</span><span class="no">EOF
  
</span><span class="c">#执行修改的桥接网络设置</span>
<span class="nv">$ </span>sysctl <span class="nt">-p</span> /etc/sysctl.d/k8s.conf
  
<span class="c"># 验证桥接的参数</span>
<span class="nv">$ </span><span class="nb">ls</span> /proc/sys/net/bridge
bridge-nf-call-arptables  bridge-nf-call-iptables        bridge-nf-filter-vlan-tagged
bridge-nf-call-ip6tables  bridge-nf-filter-pppoe-tagged  bridge-nf-pass-vlan-input-dev
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="42-配置yum源">4.2. 配置yum源</h3>

<p>需要配置的有Docker，k8s和epel的源，我们都使用马云爸爸提供的源就好了。</p>

<ul>
  <li>CentOS7/RHEL7</li>
</ul>

<p>Docker源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 安装所有yum的相关组件</span>
<span class="nv">$ </span>yum install <span class="nt">-y</span> yum-utils
<span class="c"># 把repo文件添加到本地</span>
<span class="nv">$ </span>yum-config-manager <span class="nt">--add-repo</span> http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</code></pre></div></div>

<p>epel源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>rpm <span class="nt">-ivh</span> https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm
</code></pre></div></div>

<p>k8s源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt;  /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span><span class="no">EOF
</span></code></pre></div></div>

<ul>
  <li>CentOS8/RHEL8</li>
</ul>

<p>Docker源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 安装所有yum的相关组件</span>
<span class="nv">$ </span>dnf install <span class="nt">-y</span> yum-utils
<span class="c"># 把repo文件添加到本地</span>
<span class="nv">$ </span>yum-config-manager <span class="nt">--add-repo</span> http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</code></pre></div></div>

<p>epel源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>rpm <span class="nt">-ivh</span> https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm
</code></pre></div></div>

<p>k8s源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span><span class="no">EOF
</span></code></pre></div></div>

<h3 id="43-安装软件">4.3. 安装软件</h3>

<ul>
  <li>Docker</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum <span class="nt">-y</span> install docker-ce
<span class="nv">$ </span>systemctl start docker
</code></pre></div></div>

<ul>
  <li>注意1：Centos7或者RHEL7在安装docker的时候有可能会出现依赖的报错</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>错误：软件包：3:docker-ce-19.03.7-3.el7.x86_64 <span class="o">(</span>docker-ce-stable<span class="o">)</span>
          需要：container-selinux <span class="o">&gt;=</span> 2:2.74
错误：软件包：containerd.io-1.2.13-3.1.el7.x86_64 <span class="o">(</span>docker-ce-stable<span class="o">)</span>
          需要：container-selinux <span class="o">&gt;=</span> 2:2.74
 您可以尝试添加 <span class="nt">--skip-broken</span> 选项来解决该问题
</code></pre></div></div>

<p>只需要安装上这个包就可以了</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>rpm <span class="nt">-ivh</span> http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm
</code></pre></div></div>

<ul>
  <li>注意2：RHEL8在安装docker的时候会报依赖的错误</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error:
 Problem: package docker-ce-3:19.03.8-3.el7.x86_64 requires containerd.io <span class="o">&gt;=</span> 1.2.2-3, but none of the providers can be installed
  - cannot install the best candidate <span class="k">for </span>the job
  - package containerd.io-1.2.10-3.2.el7.x86_64 is excluded
  - package containerd.io-1.2.13-3.1.el7.x86_64 is excluded
  - package containerd.io-1.2.2-3.3.el7.x86_64 is excluded
  - package containerd.io-1.2.2-3.el7.x86_64 is excluded
  - package containerd.io-1.2.4-3.1.el7.x86_64 is excluded
  - package containerd.io-1.2.5-3.1.el7.x86_64 is excluded
  - package containerd.io-1.2.6-3.3.el7.x86_64 is excluded
<span class="o">(</span>try to add <span class="s1">'--skip-broken'</span> to skip uninstallable packages or <span class="s1">'--nobest'</span> to use not only best candidate packages<span class="o">)</span>
</code></pre></div></div>

<p>只需要安装上这个包就可以了</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dnf <span class="nt">-y</span> install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm
</code></pre></div></div>

<ul>
  <li>kubectl，kubelet和kubeadm</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum <span class="nt">-y</span> install kubectl kubelet kubeadm
</code></pre></div></div>

<p>有可能出现gpg检查失败的情况，使用下面的命令安装</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum install <span class="nt">-y</span> <span class="nt">--nogpgcheck</span> kubelet kubeadm kubectl
</code></pre></div></div>

<h3 id="44-修改docker的源为国内的源">4.4. 修改Docker的源为国内的源</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> /etc/docker/daemon.json <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
{
  "registry-mirrors": ["https://gvfjy25r.mirror.aliyuncs.com"]
}
</span><span class="no">EOF
</span></code></pre></div></div>

<p>记得<code class="highlighter-rouge">systemctl daemon-reload</code>和<code class="highlighter-rouge">systemctl restart docker</code></p>

<h3 id="45-负载均衡器">4.5. 负载均衡器</h3>

<p>负载均衡可以选择Nginx，Haproxy，lvs或者traefik甚至apache都可以，基本上所有的4层负载均衡或者7层负载均衡都可以，负载均衡的主要作用就是前端使用一个统一的IP地址，后端映射api-server。让每个node通讯的时候，都通过负载均衡器来调度请求。</p>

<p>这里，我们就使用最常见，最容器实现的nginx来做负载均衡。</p>

<p><strong><em>下面的步骤在负载均衡节点master-lb上做</em></strong></p>

<ul>
  <li>安装nginx</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum <span class="nt">-y</span> install nginx
</code></pre></div></div>

<ul>
  <li>在<code class="highlighter-rouge">/etc/nginx/nginx.conf</code>里面添加一个include，让nginx读取目录下的配置文件</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>include /etc/nginx/conf.d/tcp.d/<span class="k">*</span>.conf<span class="p">;</span>
</code></pre></div></div>

<ul>
  <li>添加kubernetes的4层代理配置文件<code class="highlighter-rouge">/etc/nginx/conf.d/tcp.d/kube-api-server.conf</code></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>stream <span class="o">{</span>
    log_format main <span class="s1">'$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent'</span><span class="p">;</span>
    access_log /var/log/nginx/k8s-access.log main<span class="p">;</span>
    upstream k8s-apiserver <span class="o">{</span>
        server 10.1.1.11:6443<span class="p">;</span>
        server 10.1.1.12:6443<span class="p">;</span>
        server 10.1.1.13:6443<span class="p">;</span>
    <span class="o">}</span>
    server <span class="o">{</span>
        listen 10.1.1.10:6443<span class="p">;</span>
        proxy_pass k8s-apiserver<span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>查看端口是否在监听了</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>netstat <span class="nt">-untlp</span>|grep 6443
tcp        0      0 10.0.1.157:6443         0.0.0.0:<span class="k">*</span>               LISTEN      15787/nginx: master
</code></pre></div></div>

<p><strong><em>上面的步骤在负载均衡节点master-lb上做</em></strong></p>

<h3 id="46-使用kubeadm初始化第一个master节点">4.6. 使用kubeadm初始化第一个master节点</h3>

<ul>
  <li>检查网络是否通畅(使用telnet也可以)</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nc <span class="nt">-v</span> 10.0.1.157 6443
Ncat: Version 7.70 <span class="o">(</span> https://nmap.org/ncat <span class="o">)</span>
Ncat: Connected to 10.0.1.157:6443.
</code></pre></div></div>

<ul>
  <li>把LOAD_BALANCER_DNS:LOAD_BALANCER_PORT替换成刚才nginx的IP和端口</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm init <span class="nt">--control-plane-endpoint</span> <span class="s2">"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"</span> <span class="nt">--upload-certs</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm init <span class="nt">--control-plane-endpoint</span> <span class="s2">"10.0.1.157:6443"</span> <span class="nt">--upload-certs</span> <span class="nt">--pod-network-cidr</span><span class="o">=</span>192.168.0.0/16 <span class="nt">--image-repository</span> registry.cn-hangzhou.aliyuncs.com/google_containers
</code></pre></div></div>

<p>成功之后，会有下面的提示，找个小本本记下来吧</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir <span class="nt">-p</span> <span class="nv">$HOME</span>/.kube
  <span class="nb">sudo </span>cp <span class="nt">-i</span> /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
  <span class="nb">sudo </span>chown <span class="k">$(</span>id <span class="nt">-u</span><span class="k">)</span>:<span class="k">$(</span>id <span class="nt">-g</span><span class="k">)</span> <span class="nv">$HOME</span>/.kube/config

You should now deploy a pod network to the cluster.
Run <span class="s2">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following <span class="nb">command </span>on each as root:

  kubeadm join 10.0.1.157:6443 <span class="nt">--token</span> yww7fd.oxn7vj6484ye5amq <span class="se">\</span>
    <span class="nt">--discovery-token-ca-cert-hash</span> sha256:94f5eaf1fcd4c32d52084515d5917561fd94b2e5489798e3d2a7edd615fe892b <span class="se">\</span>
    <span class="nt">--control-plane</span> <span class="nt">--certificate-key</span> ac9317ed86ae2132d172f3dae66163f156aa13eaba299c6d00c7228abbaec9d4

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted <span class="k">in </span>two hours<span class="p">;</span> If necessary, you can use
<span class="s2">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.1.157:6443 <span class="nt">--token</span> yww7fd.oxn7vj6484ye5amq <span class="se">\</span>
    <span class="nt">--discovery-token-ca-cert-hash</span> sha256:94f5eaf1fcd4c32d52084515d5917561fd94b2e5489798e3d2a7edd615fe892b
</code></pre></div></div>

<h3 id="47-初始化master2和master3">4.7. 初始化master2和master3</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm join 10.0.1.157:6443 <span class="nt">--token</span> yww7fd.oxn7vj6484ye5amq <span class="se">\</span>
    <span class="nt">--discovery-token-ca-cert-hash</span> sha256:94f5eaf1fcd4c32d52084515d5917561fd94b2e5489798e3d2a7edd615fe892b <span class="se">\</span>
    <span class="nt">--control-plane</span> <span class="nt">--certificate-key</span> ac9317ed86ae2132d172f3dae66163f156aa13eaba299c6d00c7228abbaec9d4
</code></pre></div></div>

<h3 id="48-初始化worker1和worker2">4.8. 初始化worker1和worker2</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm join 10.0.1.157:6443 <span class="nt">--token</span> yww7fd.oxn7vj6484ye5amq <span class="se">\</span>
    <span class="nt">--discovery-token-ca-cert-hash</span> sha256:94f5eaf1fcd4c32d52084515d5917561fd94b2e5489798e3d2a7edd615fe892b
</code></pre></div></div>

<h3 id="49-选择一个网络方案">4.9. 选择一个网络方案</h3>

<p>我们这里就选用flannel了</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre></div></div>

<h3 id="410-验证一下成果">4.10. 验证一下成果</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@master-lb .kube]# kubectl get pod <span class="nt">-A</span>
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
kube-system   coredns-6955765f44-6w7hn          1/1     Running   0          59m
kube-system   coredns-6955765f44-vpdfg          1/1     Running   0          59m
kube-system   etcd-master1                      1/1     Running   0          59m
kube-system   etcd-master2                      1/1     Running   0          49m
kube-system   etcd-master3                      1/1     Running   0          48m
kube-system   kube-apiserver-master1            1/1     Running   0          59m
kube-system   kube-apiserver-master2            1/1     Running   0          49m
kube-system   kube-apiserver-master3            1/1     Running   0          47m
kube-system   kube-controller-manager-master1   1/1     Running   1          59m
kube-system   kube-controller-manager-master2   1/1     Running   0          49m
kube-system   kube-controller-manager-master3   1/1     Running   0          47m
kube-system   kube-flannel-ds-amd64-fkgbb       1/1     Running   0          11s
kube-system   kube-flannel-ds-amd64-l6rx4       1/1     Running   0          11s
kube-system   kube-flannel-ds-amd64-lhg9p       1/1     Running   0          11s
kube-system   kube-flannel-ds-amd64-rjr9x       1/1     Running   0          11s
kube-system   kube-flannel-ds-amd64-sb955       1/1     Running   0          11s
kube-system   kube-proxy-2h4dt                  1/1     Running   0          49m
kube-system   kube-proxy-67f8b                  1/1     Running   0          59m
kube-system   kube-proxy-f9774                  1/1     Running   0          50m
kube-system   kube-proxy-sw4k5                  1/1     Running   0          48m
kube-system   kube-proxy-tst5s                  1/1     Running   0          51m
kube-system   kube-scheduler-master1            1/1     Running   1          59m
kube-system   kube-scheduler-master2            1/1     Running   0          49m
kube-system   kube-scheduler-master3            1/1     Running   0          47m
</code></pre></div></div>



      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>
